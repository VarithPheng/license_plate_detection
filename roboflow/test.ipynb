{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pytesseract\n",
    "import re\n",
    "\n",
    "# Load your trained YOLOv8 model\n",
    "model = YOLO('../model/yolo.pt')  # Path to your trained weights\n",
    "\n",
    "# Define the directories for input images and cropped plates\n",
    "image_folder = \"../images\"  # Replace with your folder path\n",
    "output_folder = \"test_output\"  # Folder to save cropped plate images\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Tesseract configuration for optimal performance\n",
    "custom_config = r'--oem 3 --psm 6'\n",
    "cleaned_output_file = \"all_cleaned_texts.txt\"\n",
    "\n",
    "# Function to clean unwanted symbols and trailing characters\n",
    "def clean_text(text):\n",
    "    cleaned = re.sub(r'[^a-zA-Z0-9\\u1780-\\u17FF\\s-]', '', text)\n",
    "    cleaned = re.sub(r'\\s+[áž“]*$', '', cleaned.strip())\n",
    "    return cleaned\n",
    "\n",
    "# Function to extract text from an image using Tesseract\n",
    "def extract_text_from_image(image):\n",
    "    image = cv2.resize(image, None, fx=2, fy=2, interpolation=cv2.INTER_LINEAR)\n",
    "    blurred = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "    gray = cv2.cvtColor(blurred, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.adaptiveThreshold(\n",
    "        gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2\n",
    "    )\n",
    "    raw_text = pytesseract.image_to_string(thresh, config=custom_config, lang='khm+eng')\n",
    "    lines = raw_text.strip().split('\\n')\n",
    "    cleaned_lines = [clean_text(line) for line in lines if clean_text(line)]\n",
    "    return \"\\n\".join(cleaned_lines)\n",
    "\n",
    "# Process each image in the input folder\n",
    "with open(cleaned_output_file, \"w\", encoding=\"utf-8\") as cleaned_f:\n",
    "    for image_file in os.listdir(image_folder):\n",
    "        if image_file.endswith(('.jpg', '.png', '.jpeg')):\n",
    "            image_path = os.path.join(image_folder, image_file)\n",
    "            image = cv2.imread(image_path)\n",
    "\n",
    "            # Perform detection using YOLOv8\n",
    "            results = model(image)[0]\n",
    "            xyxy = results.boxes.xyxy.numpy()\n",
    "            confidences = results.boxes.conf.numpy()\n",
    "            class_ids = results.boxes.cls.numpy().astype(int)\n",
    "\n",
    "            detections = sv.Detections(xyxy=xyxy, confidence=confidences, class_id=class_ids)\n",
    "            bounding_box_annotator = sv.BoxAnnotator()\n",
    "            annotated_image = bounding_box_annotator.annotate(scene=image, detections=detections)\n",
    "\n",
    "            # Iterate over detected plates and extract text\n",
    "            for idx, box in enumerate(xyxy):\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                plate_image = image[y1:y2, x1:x2]  # Crop the plate from the image\n",
    "                cropped_path = os.path.join(output_folder, f\"{image_file}_plate_{idx}.jpg\")\n",
    "                cv2.imwrite(cropped_path, plate_image)  # Save the cropped plate image\n",
    "\n",
    "                # Extract text from the cropped plate\n",
    "                cleaned_text = extract_text_from_image(plate_image)\n",
    "                cleaned_f.write(f\"\\n[Cleaned Text from {image_file}_plate_{idx}]:\\n{cleaned_text}\\n\")\n",
    "\n",
    "            # Display the annotated image\n",
    "            sv.plot_image(image=annotated_image, size=(16, 16))\n",
    "\n",
    "print(f\"Processing completed. Extracted texts are saved in {cleaned_output_file}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "import imutils\n",
    "\n",
    "# Optional: Set the Tesseract executable path if you are on Windows\n",
    "# pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "def extract_license_plate_text(image_path):\n",
    "    # Step 1: Load the image and resize for consistency\n",
    "    image = cv2.imread(image_path)\n",
    "    image = imutils.resize(image, width=600)\n",
    "\n",
    "    # Step 2: Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Step 3: Apply bilateral filter to reduce noise while keeping edges sharp\n",
    "    filtered = cv2.bilateralFilter(gray, 11, 17, 17)\n",
    "\n",
    "    # Step 4: Use adaptive thresholding for better text visibility\n",
    "    thresh = cv2.adaptiveThreshold(filtered, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                    cv2.THRESH_BINARY, 11, 2)\n",
    "\n",
    "    # Step 5: Detect edges using Canny\n",
    "    edges = cv2.Canny(thresh, 30, 200)\n",
    "\n",
    "    # Step 6: Find contours and sort by area\n",
    "    contours = cv2.findContours(edges.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours = imutils.grab_contours(contours)\n",
    "    contours = sorted(contours, key=cv2.contourArea, reverse=True)[:10]\n",
    "\n",
    "    # Step 7: Loop through contours to find a 4-point contour (possible plate)\n",
    "    plate = None\n",
    "    for contour in contours:\n",
    "        perimeter = cv2.arcLength(contour, True)\n",
    "        approx = cv2.approxPolyDP(contour, 0.02 * perimeter, True)\n",
    "\n",
    "        # Check for 4-cornered contour (rectangle or trapezoid)\n",
    "        if len(approx) == 4:\n",
    "            plate = approx\n",
    "            break\n",
    "\n",
    "    # Step 8: Extract the region of interest (ROI) if plate-like contour is found\n",
    "    if plate is not None:\n",
    "        x, y, w, h = cv2.boundingRect(plate)\n",
    "        roi = gray[y:y+h, x:x+w]\n",
    "\n",
    "        # Step 9: Apply Otsu's threshold to improve text visibility\n",
    "        roi = cv2.threshold(roi, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "        # Optional: Display the ROI to verify clarity\n",
    "        cv2.imshow(\"ROI\", roi)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        # Step 10: Perform OCR using Tesseract with Khmer + English languages\n",
    "        config = '--psm 7'  # Assume a single line of text\n",
    "        text = pytesseract.image_to_string(roi, config=config, lang='khm+eng')\n",
    "\n",
    "        return text.strip()\n",
    "    else:\n",
    "        return \"License plate not detected.\"\n",
    "\n",
    "# Test the function with your image path\n",
    "image_path = 'images/71039975-Phnom-Penh-license-plate-Phnom-Penh-Phnom-Penh-Cambodia-Asia.jpg'\n",
    "text = extract_license_plate_text(image_path)\n",
    "print(\"Detected Text:\", text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
